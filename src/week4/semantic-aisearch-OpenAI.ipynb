{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install bs4 chardet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environments setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Tuple\n",
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion, OpenAITextEmbedding, AzureChatCompletion, AzureTextEmbedding\n",
    "\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_org_id = os.getenv(\"OPENAI_ORG_ID\")\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "useAzureOpenAI = False\n",
    "\n",
    "# Configure AI service used by the kernel\n",
    "if useAzureOpenAI:\n",
    "    kernel.add_chat_service(\"chat_completion\", AzureChatCompletion(\"startping_deploymodel\", endpoint, api_key))\n",
    "    kernel.add_text_embedding_generation_service(\"ada\", AzureTextEmbedding(\"startping_embedding\", endpoint, api_key))\n",
    "else:\n",
    "    kernel.add_chat_service(\"chat-gpt\", OpenAIChatCompletion(\"gpt-3.5-turbo\", openai_api_key, openai_org_id))\n",
    "    kernel.add_text_embedding_generation_service(\"ada\", OpenAITextEmbedding(\"text-embedding-ada-002\", openai_api_key, openai_org_id))\n",
    "\n",
    "\n",
    "kernel.register_memory_store(memory_store=sk.memory.VolatileMemoryStore())\n",
    "kernel.import_skill(sk.core_skills.TextMemorySkill())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading html file to memory vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import chardet\n",
    "\n",
    "file_path = '../../data/html/wmt-20230430.html'\n",
    "\n",
    "def load_local_html(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        result = chardet.detect(file.read())\n",
    "\n",
    "    encoding = result['encoding']\n",
    "    # print(encoding)\n",
    "    \n",
    "    with open(file_path, 'r', encoding=encoding) as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "soup = load_local_html(file_path)\n",
    "\n",
    "# Now you can use BeautifulSoup methods to navigate and extract information from the HTML\n",
    "# For example, print the text content of all paragraphs\n",
    "outfile_path = '../../data/txt/wmt-20230430.txt'\n",
    "with open(outfile_path, 'w', encoding='utf-8') as file:\n",
    "    for paragraph in soup.find_all('span'):\n",
    "        file.write(paragraph.get_text()+'\\n\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "loader = TextLoader(\"../../data/txt/wmt-20230430.txt\")\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=5000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs[1].page_content\n",
    "memory_collection_name = \"wmt10Q12023\"\n",
    "print(\"Adding WalMart 2023 Quarter Report and their descriptions to a volatile Semantic Memory.\");\n",
    "i = 0\n",
    "for entry, value in enumerate(docs):\n",
    "    await kernel.memory.save_reference_async(\n",
    "        collection=memory_collection_name,\n",
    "        description=value.page_content,\n",
    "        text=value.page_content,\n",
    "        external_id=entry,\n",
    "        external_source_name=\"wmt10Q12023\"\n",
    "    )\n",
    "    i += 1\n",
    "    print(\"  URL {} saved\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask = \"\"\"find context relevent to Consolidated net income\"\"\"\n",
    "# ask = \"\"\"Rating agency for Long-term debt\"\"\"\n",
    "ask = \"\"\" Are there any legal proceedings? If yes, please summarize each legal proceeding in one paragraph with no more than five sentences.\"\"\"\n",
    "# ask = \"\"\"find context relevent to cash equivalents and restricted cash at end of period\"\"\"\n",
    "\n",
    "debugs = await kernel.memory.search_async(memory_collection_name, ask, limit=1, min_relevance_score=0.77)\n",
    "print(debugs[0].relevance)\n",
    "print(debugs[0].description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Native Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "async def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # a parameter that controls the randomness or creativity of the generated text, examples are 0,1,0.5\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatGPTSearchEngineSkill:\n",
    "    \"\"\"\n",
    "    A ChatGPT search engine skill.\n",
    "    \"\"\"\n",
    "    from semantic_kernel.orchestration.sk_context import SKContext\n",
    "    from semantic_kernel.skill_definition import sk_function, sk_function_context_parameter\n",
    "\n",
    "    def __init__(self, connector) -> None:\n",
    "        self._connector = connector\n",
    "\n",
    "    @sk_function(\n",
    "        description=\"Performs a ChatGPT search on sector according to the Global Industry Classification Standard (GICS)\", name=\"searchAsync\"\n",
    "    )\n",
    "    @sk_function_context_parameter(\n",
    "        name=\"query\",\n",
    "        description=\"The search query\",\n",
    "    )\n",
    "    async def search_async(self, query: str, context: SKContext) -> str:\n",
    "        query = query or context.variables.get(\"query\")\n",
    "        result = await self._connector(query)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebSearchEngineSkill:\n",
    "    \"\"\"\n",
    "    A search engine skill.\n",
    "    \"\"\"\n",
    "    from semantic_kernel.orchestration.sk_context import SKContext\n",
    "    from semantic_kernel.skill_definition import sk_function, sk_function_context_parameter\n",
    "\n",
    "    def __init__(self, connector) -> None:\n",
    "        self._connector = connector\n",
    "\n",
    "    @sk_function(\n",
    "        description=\"Performs NEGATIVE news web search for a given query\", name=\"searchAsync\"\n",
    "    )\n",
    "    @sk_function_context_parameter(\n",
    "        name=\"query\",\n",
    "        description=\"The search query\",\n",
    "    )\n",
    "    async def search_async(self, query: str, context: SKContext) -> str:\n",
    "        query = query or context.variables.get(\"query\")\n",
    "        # print(\"debug {query}\")\n",
    "        result = await self._connector.search_async(query, num_results=5, offset=0)\n",
    "        return str(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorSearchEngineSkill:\n",
    "    \"\"\"\n",
    "    A vector search engine skill.\n",
    "    \"\"\"\n",
    "    from semantic_kernel.orchestration.sk_context import SKContext\n",
    "    from semantic_kernel.skill_definition import sk_function, sk_function_context_parameter\n",
    "\n",
    "    @sk_function(\n",
    "        description=\"find context relevent to Consolidated net income or \\\n",
    "            Rating agency for Long-term debt or \\\n",
    "            legal proceedings or \\\n",
    "            cash equivalents and restricted cash at end of period \\\n",
    "            \", name=\"searchAsync\"\n",
    "    )\n",
    "    @sk_function_context_parameter(\n",
    "        name=\"query\",\n",
    "        description=\"The search query\",\n",
    "    )\n",
    "    async def search_async(self, query: str, context: SKContext) -> str:\n",
    "        query = query or context.variables.get(\"query\")\n",
    "        ask = f\"find context relevent to {query}\"\n",
    "        # print(f\"debug query: {ask}\")\n",
    "        # msg= context.variables.get(\"query\")\n",
    "        # print(f\"debug msg: {query}\")\n",
    "\n",
    "        memories = await kernel.memory.search_async(memory_collection_name, ask, limit=1, min_relevance_score=0.77)\n",
    "        result = memories[0].description\n",
    "        # print(memories[0].relevance)\n",
    "        # print(f\"debug: {result}\")\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.search_engine import BingConnector\n",
    "\n",
    "BING_API_KEY = os.getenv(\"BING_SEARCH_V7_SUBSCRIPTION_KEY\")\n",
    "connector = BingConnector(BING_API_KEY)\n",
    "\n",
    "kernel.import_skill(VectorSearchEngineSkill(), skill_name=\"vectorsearch\")\n",
    "kernel.import_skill(WebSearchEngineSkill(connector), skill_name=\"websearch\")\n",
    "kernel.import_skill(ChatGPTSearchEngineSkill(get_completion), skill_name=\"chatgptsearch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepwise Planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.planning import StepwisePlanner\n",
    "from semantic_kernel.planning.stepwise_planner.stepwise_planner_config import (\n",
    "    StepwisePlannerConfig,\n",
    ")\n",
    "\n",
    "planner = StepwisePlanner(\n",
    "    kernel, StepwisePlannerConfig(max_iterations=2, min_iteration_time_ms=1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good Questions\n",
    "```\n",
    "ask = \"\"\"find context relevent to Consolidated net income\"\"\"\n",
    "ask = \"\"\"Rating agency for Long-term debt\"\"\"\n",
    "ask = \"\"\"find legal proceedings\"\"\"\n",
    "ask = \"\"\"find context relevent to cash equivalents and restricted cash at end of period\"\"\"\n",
    "ask = \"\"\"What is the sector of Walmart according to the Global Industry Classification Standard (GICS)?\"\"\"\n",
    "ask = \"\"\"Are there any negatives news about Walmart in last 12 months?\"\"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask = \"\"\"Rating agency for Long-term debt\"\"\"\n",
    "plan = planner.create_plan(goal=ask)\n",
    "\n",
    "for step in plan._steps:\n",
    "    print(step.description, \":\", step._state.__dict__)\n",
    "\n",
    "result = await plan.invoke_async()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"===========================\\n\" + \"Query: \" + ask + \"\\n\")\n",
    "print(\"===========================\\n\" + \"Result: \\n\")\n",
    "print(result)\n",
    "print(\"===========================\\n\" + \"Thoughts: \\n\")\n",
    "for index, step in enumerate(plan._steps):\n",
    "    print(\"Step:\", index)\n",
    "    print(\"Description:\",step.description)\n",
    "    print(\"Function:\", step.skill_name + \".\" + step._function.name)\n",
    "    if len(step._outputs) > 0:\n",
    "        print( \"  Output:\\n\", str.replace(result[step._outputs[0]],\"\\n\", \"\\n  \"))\n",
    "print(\"============END===============\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ask = \"\"\"\n",
    "Could you please find relevent information for the following questions for WalMart \\\n",
    "Provide answer to question 1, once finish, answer to question 2 , and then answer to question 3\\\n",
    "\n",
    "question 1: find context relevent to Consolidated net income\\\n",
    "question 2: Rating agency for Long-term debt\\\n",
    "question 3: find legal proceedings\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " the stepwise planner allows for the AI to form \"thoughts\" and \"observations\" and execute actions based off those to achieve a user's goal. This continues until all required functions are complete and a final output is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask = \"\"\"Are there any negatives news about Walmart in last 12 months?\"\"\"\n",
    "async def execute_plan(ask):\n",
    "    plan = planner.create_plan(goal=ask)\n",
    "\n",
    "    # for step in plan._steps:\n",
    "    #     print(step.description, \":\", step._state.__dict__)\n",
    "\n",
    "    result = await plan.invoke_async()\n",
    "    print(\"===========================\\n\" + \"Query: \" + ask + \"\\n\")\n",
    "    print(\"===========================\\n\" + \"Result: \\n\")\n",
    "    print(result)\n",
    "    print(\"===========================\\n\" + \"Thoughts: \\n\")\n",
    "    for index, step in enumerate(plan._steps):\n",
    "        print(\"Step:\", index)\n",
    "        print(\"Description:\",step.description)\n",
    "        print(\"Function:\", step.skill_name + \".\" + step._function.name)\n",
    "        if len(step._outputs) > 0:\n",
    "            print( \"  Output:\\n\", str.replace(result[step._outputs[0]],\"\\n\", \"\\n  \"))\n",
    "    print(\"============END===============\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include all questions\n",
    "async def execute_project(questions) -> None:\n",
    "    for question in questions:\n",
    "        await execute_plan(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    # \"Are there any negatives news about Walmart in last 12 months?\",\n",
    "    # \"What is the sector of Walmart according to the Global Industry Classification Standard (GICS)?\",\n",
    "    \"find context relevent to amount for {cash equivalents and restricted cash at end of period} as of April 30, 2023 \",\n",
    "    \"find context relevent to amount for {Consolidated net income} as of April 30, 2023\",\n",
    "    \"find context relevent to information for {legal proceedings or certain regulatory matters}\",\n",
    "    \"find context relevent to Rating for {Rating agency for Long-term debt}\",\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await execute_project(questions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gptold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
