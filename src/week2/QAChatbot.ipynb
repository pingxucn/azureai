{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Q and A Chatbot using Assistants API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Messages\n",
      "assistant: The title of the paper is \"Retrieval Augmented Generation and Representative Vector Summarization for Large Unstructured Textual Data in Medical Education\".\n",
      "user: what's the tile of the paper?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import openai\n",
    "\n",
    "\n",
    "def show_json(obj):\n",
    "    display(json.loads(obj.model_dump_json())) \n",
    "\n",
    "# Pretty printing helper\n",
    "def pretty_print(messages):\n",
    "    print(\"# Messages\")\n",
    "    for m in messages:\n",
    "        print(f\"{m.role}: {m.content[0].text.value}\")\n",
    "    print()\n",
    "\n",
    "def wait_on_run(run, thread):\n",
    "    while run.status == \"queued\" or run.status == \"in_progress\":\n",
    "        run = client.beta.threads.runs.retrieve(\n",
    "            thread_id=thread.id,\n",
    "            run_id=run.id,\n",
    "        )\n",
    "        time.sleep(0.5)\n",
    "    return run\n",
    "\n",
    "client = openai.OpenAI(api_key = os.getenv(\"OPENAI_API_KEY\"))\n",
    "file_path = \"..\\\\..\\\\data\\\\pdf\\\\test\\\\2308.00479.pdf\"\n",
    "user_prompt = \"what's the tile of the paper?\"\n",
    "\n",
    "# Upload a file with an \"assistants\" purpose\n",
    "file = client.files.create(\n",
    "  file=open(file_path, \"rb\"),\n",
    "  purpose='assistants'\n",
    ")\n",
    "\n",
    "# Add the file to the assistant\n",
    "# You can attach a maximum of 20 files per Assistant, and they can be at most 512 MB each. \n",
    "# In addition, the size of all the files uploaded by your organization should not exceed 100GB. \n",
    "# You can request an increase in this storage limit using our help center.\n",
    "assistant = client.beta.assistants.create(\n",
    "  instructions=\"You are a customer support chatbot. Use your knowledge base to best respond to customer queries.\",\n",
    "  model=\"gpt-4-1106-preview\",\n",
    "  tools=[{\"type\": \"retrieval\"}],\n",
    "  file_ids=[file.id]\n",
    ")\n",
    "\n",
    "# thread = client.beta.threads.create()\n",
    "\n",
    "# message = client.beta.threads.messages.create(\n",
    "#   thread_id=thread.id,\n",
    "#   role=\"user\",\n",
    "#   content=user_prompt,\n",
    "#   file_ids=[file.id]\n",
    "# )\n",
    "\n",
    "thread = client.beta.threads.create(\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": user_prompt,\n",
    "      \"file_ids\": [file.id]\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "\n",
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    ")\n",
    "\n",
    "run = wait_on_run(run, thread)\n",
    "messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "\n",
    "pretty_print(messages)\n",
    "# messages.data[0].content[0].text.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "MATH_ASSISTANT_ID = assistant.id  # or a hard-coded ID like \"asst-...\"\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def submit_message(assistant_id, thread, user_message):\n",
    "    client.beta.threads.messages.create(\n",
    "        thread_id=thread.id, role=\"user\", content=user_message\n",
    "    )\n",
    "    return client.beta.threads.runs.create(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=assistant_id,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_response(thread):\n",
    "    return client.beta.threads.messages.list(thread_id=thread.id, order=\"asc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_thread_and_run(user_input):\n",
    "    thread = client.beta.threads.create()\n",
    "    run = submit_message(MATH_ASSISTANT_ID, thread, user_input)\n",
    "    run = wait_on_run(run, thread)\n",
    "    pretty_print(get_response(thread))\n",
    "\n",
    "# create_thread_and_run(\"what's the tile of the paper?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You typed: list authors of the paper\n",
      "# Messages\n",
      "user: list authors of the paper\n",
      "assistant: The authors of the paper are:\n",
      "\n",
      "- S. S. Manathunga, Department of Pharmacology, University of Peradeniya, Sri Lanka (ssm123ssm@gmail.com)\n",
      "- Y. A. Illangasekera, Department of Pharmacology, University of Peradeniya, Sri Lanka (yasi04@gmail.com)【7†source】.\n",
      "\n",
      "You typed: Can you summarize the paper \n",
      "# Messages\n",
      "user: Can you summarize the paper \n",
      "assistant: The paper titled \"Retrieval Augmented Generation and Representative Vector Summarization for Large Unstructured Textual Data in Medical Education\" discusses how Large Language Models (LLMs) are increasingly being utilized for content generation and chatbots. However, these models must be specifically aligned when applied to domain-specific tasks to avoid issues such as producing incorrect or harmful answers. A technique called Retrieval Augmented Generation (RAG) is proposed, which facilitates the easy attachment and manipulation of non-parametric knowledge bases to LLMs, making them more suitable for domain-specific applications, particularly medical education. The paper proposes a novel summarization method that combines extractive and abstractive approaches utilizing representative vectors, tailored to work with large unstructured textual data【7†source】.\n",
      "\n",
      "Exiting the program. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "input_instructions = f\"\"\"I am a customer support chatbot to answer questions related to {file_path},\n",
    "    you can type your questions, or type Bye to exit:\n",
    "\"\"\"\n",
    "while True:\n",
    "    user_input = input(input_instructions)\n",
    "    \n",
    "    if \"bye\" in user_input.lower():\n",
    "        print(\"Exiting the program. Goodbye!\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"You typed:\", user_input)\n",
    "        create_thread_and_run(user_input)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
