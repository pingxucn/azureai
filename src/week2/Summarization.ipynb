{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use OpenAI knowledge-retrieval gpt-4-1106-preview\n",
    "Retrieval augments the Assistant with knowledge from outside its model, such as proprietary product information or documents provided by your users. Once a file is uploaded and passed to the Assistant, OpenAI will automatically chunk your documents, index and store the embeddings, and implement vector search to retrieve relevant content to answer user queries.\\\n",
    "https://platform.openai.com/docs/assistants/tools/knowledge-retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Messages\n",
      "assistant: The article discusses the application of Retrieval Augmented Generation (RAG) enhanced by Representative Vector Summarization (RVS) for unstructured textual data, particularly in the context of medical education. Large Language Models (LLMs), known for their impressive zero-shot learning capabilities in various tasks including content generation and chatbot functionality, can run into issues when applied to domain-specific tasks, such as producing factually incorrect information or being difficult to update once trained on a vast corpus of data.\n",
      "\n",
      "To address these challenges, the authors propose a combined extractive and abstractive summarization method using representative vectors that improves upon direct summarization by existing LLMs, which may struggle with large documents due to context window limitations and the \"Lost in the Middle\" problem where facts in the middle of the context are overlooked. RVS selects a predefined number of representative chunks of text from a non-parametric knowledgebase, using k-means clustering and high-dimensional vectors to identify key concepts within the document.\n",
      "\n",
      "The process commences with extraction of text from multiple unstructured sources, then the text is broken into chunks and converted into high-dimensional vectors to be stored in a FAISS vector database. The document referred in the article has been implemented with the program 'docGPT,' which is built in the Python programming language using the langchain framework.\n",
      "\n",
      "Additionally, docGPT can create visual representations like word clouds and scatter plots based on keywords and their relative significance to provide an overview of the document's content. The authors present evaluations demonstrating that docGPT, with a RAG model and vector databases, provides more targeted and accurate answers to medical queries compared to LLMs without such augmentation.\n",
      "\n",
      "The article was authored by S. S. Manathunga and Y. A. Illangasekera from the Department of Pharmacology at the University of Peradeniya, Sri Lanka. \n",
      "\n",
      "For a more complete understanding, it's suggested to visit the supplemental materials referred to in the article for details on specific queries and summarization performance comparisons using medical reference texts. The source code for docGPT is made available publicly【9†(source)】.\n",
      "user: Could you please summarize the article?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import openai\n",
    "\n",
    "\n",
    "def show_json(obj):\n",
    "    display(json.loads(obj.model_dump_json())) \n",
    "\n",
    "# Pretty printing helper\n",
    "def pretty_print(messages):\n",
    "    print(\"# Messages\")\n",
    "    for m in messages:\n",
    "        print(f\"{m.role}: {m.content[0].text.value}\")\n",
    "    print()\n",
    "\n",
    "def wait_on_run(run, thread):\n",
    "    while run.status == \"queued\" or run.status == \"in_progress\":\n",
    "        run = client.beta.threads.runs.retrieve(\n",
    "            thread_id=thread.id,\n",
    "            run_id=run.id,\n",
    "        )\n",
    "        time.sleep(0.5)\n",
    "    return run\n",
    "\n",
    "client = openai.OpenAI(api_key = os.getenv(\"OPENAI_API_KEY\"))\n",
    "file_path = \"..\\\\..\\\\data\\\\pdf\\\\test\\\\2308.00479.pdf\"\n",
    "user_prompt = \"Could you please summarize the article?\"\n",
    "\n",
    "# Upload a file with an \"assistants\" purpose\n",
    "file = client.files.create(\n",
    "  file=open(file_path, \"rb\"),\n",
    "  purpose='assistants'\n",
    ")\n",
    "\n",
    "# Add the file to the assistant\n",
    "# You can attach a maximum of 20 files per Assistant, and they can be at most 512 MB each. \n",
    "# In addition, the size of all the files uploaded by your organization should not exceed 100GB. \n",
    "# You can request an increase in this storage limit using our help center.\n",
    "assistant = client.beta.assistants.create(\n",
    "  instructions=\"You are a customer support chatbot. Use your knowledge base to best respond to customer queries.\",\n",
    "  model=\"gpt-4-1106-preview\",\n",
    "  tools=[{\"type\": \"retrieval\"}],\n",
    "  file_ids=[file.id]\n",
    ")\n",
    "\n",
    "# thread = client.beta.threads.create()\n",
    "\n",
    "# message = client.beta.threads.messages.create(\n",
    "#   thread_id=thread.id,\n",
    "#   role=\"user\",\n",
    "#   content=user_prompt,\n",
    "#   file_ids=[file.id]\n",
    "# )\n",
    "\n",
    "thread = client.beta.threads.create(\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": user_prompt,\n",
    "      \"file_ids\": [file.id]\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "\n",
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    ")\n",
    "\n",
    "run = wait_on_run(run, thread)\n",
    "messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "\n",
    "pretty_print(messages)\n",
    "# messages.data[0].content[0].text.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Azure Document Intelligence models\n",
    "https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/use-sdk-rest-api?view=doc-intel-3.0.0&preserve-view=true%3Fpivots%3Dprogramming-language-python&tabs=windows&pivots=programming-language-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 read PDF to memory\n",
    "import os\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# set `<your-endpoint>` and `<your-key>` variables with the values from the Azure portal\n",
    "endpoint = os.environ[\"AZURE_FORM_RECOGNIZER_ENDPOINT\"]\n",
    "key = os.environ[\"AZURE_FORM_RECOGNIZER_KEY\"]\n",
    "\n",
    "# Azure AI sample document\n",
    "file_path = \"..\\\\..\\\\data\\\\pdf\\\\test\\\\2308.00479.pdf\"\n",
    "# file_formUrl = \"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-REST-api-samples/master/curl/form-recognizer/rest-api/read.png\"\n",
    "\n",
    "# formatting function\n",
    "def format_polygon(polygon):\n",
    "    if not polygon:\n",
    "        return \"N/A\"\n",
    "    return \", \".join([\"[{}, {}]\".format(p.x, p.y) for p in polygon])\n",
    "\n",
    "\n",
    "def analyze_read(file_path):\n",
    "    # sample document\n",
    "\n",
    "    document_analysis_client = DocumentAnalysisClient(\n",
    "        endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    "    )\n",
    "\n",
    "    if \"https://\" in file_path:\n",
    "        poller = document_analysis_client.begin_analyze_document_from_url(\n",
    "            \"prebuilt-read\", file_path\n",
    "        )\n",
    "    else:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            poller = document_analysis_client.begin_analyze_document(\n",
    "            \"prebuilt-read\", document=f, locale=\"en-US\"\n",
    "        )\n",
    "  \n",
    "    result = poller.result()\n",
    "\n",
    "    print(\"Document contains content: \", result.content)\n",
    "\n",
    "    for idx, style in enumerate(result.styles):\n",
    "        print(\n",
    "            \"Document contains {} content\".format(\n",
    "                \"handwritten\" if style.is_handwritten else \"no handwritten\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    for page in result.pages:\n",
    "        print(\"----Analyzing Read from page #{}----\".format(page.page_number))\n",
    "        print(\n",
    "            \"Page has width: {} and height: {}, measured with unit: {}\".format(\n",
    "                page.width, page.height, page.unit\n",
    "            )\n",
    "        )\n",
    "\n",
    "        for line_idx, line in enumerate(page.lines):\n",
    "            print(\n",
    "                \"...Line # {} has text content '{}' within bounding box '{}'\".format(\n",
    "                    line_idx,\n",
    "                    line.content,\n",
    "                    format_polygon(line.polygon),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        for word in page.words:\n",
    "            print(\n",
    "                \"...Word '{}' has a confidence of {}\".format(\n",
    "                    word.content, word.confidence\n",
    "                )\n",
    "            )\n",
    "\n",
    "    print(\"----------------------------------------\")\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result = analyze_read(file_path)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries abstracted:\n",
      "The document discusses the application of Retrieval Augmented Generation (RAG) in medical education, specifically in the field of medical education. The paper introduces a combined extractive and abstractive summarization method for large unstructured textual data using representative vectors. The method, called Representative Vector Summarization (RVS), is implemented in docGPT, a document intelligence program written in Python. The document also describes the process of retrieval and summarization, and the evaluation of the methods. The authors conclude that RVS could provide efficient methods for retrieving information quickly from large knowledgebases, particularly in the field of clinical medicine.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# step 2 abtractively summarization \n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "\n",
    "\n",
    "def sample_abstractive_summarization(document) -> None:\n",
    "    # [START abstract_summary]\n",
    "    endpoint = os.environ[\"AZURE_LANGUAGE_ENDPOINT\"]\n",
    "    key = os.environ[\"AZURE_LANGUAGE_KEY\"]    \n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "        endpoint=endpoint,\n",
    "        credential=AzureKeyCredential(key),\n",
    "    )\n",
    "\n",
    "    poller = text_analytics_client.begin_abstract_summary(document)\n",
    "    abstract_summary_results = poller.result()\n",
    "    for result in abstract_summary_results:\n",
    "        if result.kind == \"AbstractiveSummarization\":\n",
    "            print(\"Summaries abstracted:\")\n",
    "            [print(f\"{summary.text}\\n\") for summary in result.summaries]\n",
    "        elif result.is_error is True:\n",
    "            print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "                result.error.code, result.error.message\n",
    "            ))\n",
    "    # [END abstract_summary]\n",
    "\n",
    "sample_abstractive_summarization([result.content])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
