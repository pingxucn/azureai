{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using document summarization\n",
    "def sample_abstractive_summarization() -> None:\n",
    "    # [START abstract_summary]\n",
    "    import os\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.ai.textanalytics import TextAnalyticsClient\n",
    "\n",
    "    endpoint = os.environ[\"AZURE_LANGUAGE_ENDPOINT\"]\n",
    "    key = os.environ[\"AZURE_LANGUAGE_KEY\"]\n",
    "\n",
    "    text_analytics_client = TextAnalyticsClient(\n",
    "        endpoint=endpoint,\n",
    "        credential=AzureKeyCredential(key),\n",
    "    )\n",
    "\n",
    "    document = [\n",
    "        \"At Microsoft, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic, \"\n",
    "        \"human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Cognitive \"\n",
    "        \"Services, I have been working with a team of amazing scientists and engineers to turn this quest into a \"\n",
    "        \"reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of \"\n",
    "        \"human cognition: monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the \"\n",
    "        \"intersection of all three, there's magic-what we call XYZ-code as illustrated in Figure 1-a joint \"\n",
    "        \"representation to create more powerful AI that can speak, hear, see, and understand humans better. \"\n",
    "        \"We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, \"\n",
    "        \"spanning modalities and languages. The goal is to have pretrained models that can jointly learn \"\n",
    "        \"representations to support a broad range of downstream AI tasks, much in the way humans do today. \"\n",
    "        \"Over the past five years, we have achieved human performance on benchmarks in conversational speech \"\n",
    "        \"recognition, machine translation, conversational question answering, machine reading comprehension, \"\n",
    "        \"and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious \"\n",
    "        \"aspiration to produce a leap in AI capabilities, achieving multisensory and multilingual learning that \"\n",
    "        \"is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational \"\n",
    "        \"component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\"\n",
    "    ]\n",
    "\n",
    "    poller = text_analytics_client.begin_abstract_summary(document)\n",
    "    abstract_summary_results = poller.result()\n",
    "    for result in abstract_summary_results:\n",
    "        if result.kind == \"AbstractiveSummarization\":\n",
    "            print(\"Summaries abstracted:\")\n",
    "            [print(f\"{summary.text}\\n\") for summary in result.summaries]\n",
    "        elif result.is_error is True:\n",
    "            print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "                result.error.code, result.error.message\n",
    "            ))\n",
    "    # [END abstract_summary]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample_abstractive_summarization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single-shot recognition speech to text\n",
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "\n",
    "def speech_to_text(audio_file_path, subscription_key, service_region):\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=subscription_key, region=service_region)\n",
    "    audio_input = speechsdk.AudioConfig(filename=audio_file_path)\n",
    "\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_input)\n",
    "    result = speech_recognizer.recognize_once_async().get()\n",
    "    # result = speech_recognizer.recognize_once()\n",
    "\n",
    "    if result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        print(\"Recognized: {}\".format(result.text))\n",
    "    elif result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        print(\"No speech could be recognized\")\n",
    "    elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = result.cancellation_details\n",
    "        print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file_path = \"..\\\\data\\\\speech\\\\BillGates_2010.wav\" \n",
    "    # audio_file_path = \"..\\\\data\\\\speech\\\\time.wav\" \n",
    "    subscription_key = os.environ[\"AZURE_SPEECH_KEY\"]\n",
    "    service_region = os.environ[\"AZURE_SPEECH_REGION\"]\n",
    "\n",
    "    speech_to_text(audio_file_path, subscription_key, service_region)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# set `<your-endpoint>` and `<your-key>` variables with the values from the Azure portal\n",
    "endpoint = os.environ[\"AZURE_FORM_RECOGNIZER_ENDPOINT\"]\n",
    "key = os.environ[\"AZURE_FORM_RECOGNIZER_KEY\"]\n",
    "\n",
    "# Azure AI sample document\n",
    "# docUrl = \"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-REST-api-samples/master/curl/form-recognizer/sample-layout.pdf\"\n",
    "# fileName='C:\\\\Repository\\\\Python\\\\VicG5\\\\data\\\\sample-layout.pdf'\n",
    "\n",
    "# Vic G5 sample document\n",
    "fileName='C:\\\\Repository\\\\Python\\\\VicG5\\\\data\\\\ResidentialLease2.pdf'\n",
    "\n",
    "document_analysis_client = DocumentAnalysisClient(\n",
    "        endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    "    )\n",
    "\n",
    "# poller = document_analysis_client.begin_analyze_document_from_url(\"prebuilt-document\", docUrl)\n",
    "\n",
    "with open(fileName, \"rb\") as f:\n",
    "  poller = document_analysis_client.begin_analyze_document(\n",
    "    \"prebuilt-document\", document=f, locale=\"en-US\"\n",
    ")\n",
    "result = poller.result()\n",
    "\n",
    "print(f\"----Key-value pairs found in document {fileName}----\")\n",
    "for kv_pair in result.key_value_pairs:\n",
    "    if kv_pair.key and kv_pair.value:\n",
    "        print(\"Key '{}': Value: '{}'\".format(kv_pair.key.content, kv_pair.value.content))\n",
    "    else:\n",
    "        print(\"Key '{}': Value:\".format(kv_pair.key.content))\n",
    "\n",
    "print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy from Test Model \n",
    "\"\"\"\n",
    "This code sample shows Custom Extraction Model operations with the Azure Form Recognizer client library. \n",
    "The async versions of the samples require Python 3.6 or later.\n",
    "\n",
    "To learn more, please visit the documentation - Quickstart: Form Recognizer Python client library SDKs\n",
    "https://docs.microsoft.com/en-us/azure/applied-ai-services/form-recognizer/quickstarts/try-v3-python-sdk\n",
    "\"\"\"\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "\n",
    "\"\"\"\n",
    "Remember to remove the key from your code when you're done, and never post it publicly. For production, use\n",
    "secure methods to store and access your credentials. For more information, see \n",
    "https://docs.microsoft.com/en-us/azure/cognitive-services/cognitive-services-security?tabs=command-line%2Ccsharp#environment-variables-and-application-configuration\n",
    "\"\"\"\n",
    "endpoint = \"YOUR_FORM_RECOGNIZER_ENDPOINT\"\n",
    "key = \"YOUR_FORM_RECOGNIZER_KEY\"\n",
    "\n",
    "model_id = \"YOUR_CUSTOM_BUILT_MODEL_ID\"\n",
    "formUrl = \"YOUR_DOCUMENT\"\n",
    "\n",
    "document_analysis_client = DocumentAnalysisClient(\n",
    "    endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    ")\n",
    "\n",
    "# Make sure your document's type is included in the list of document types the custom model can analyze\n",
    "poller = document_analysis_client.begin_analyze_document_from_url(model_id, formUrl)\n",
    "result = poller.result()\n",
    "\n",
    "for idx, document in enumerate(result.documents):\n",
    "    print(\"--------Analyzing document #{}--------\".format(idx + 1))\n",
    "    print(\"Document has type {}\".format(document.doc_type))\n",
    "    print(\"Document has confidence {}\".format(document.confidence))\n",
    "    print(\"Document was analyzed by model with ID {}\".format(result.model_id))\n",
    "    for name, field in document.fields.items():\n",
    "        field_value = field.value if field.value else field.content\n",
    "        print(\"......found field of type '{}' with value '{}' and with confidence {}\".format(field.value_type, field_value, field.confidence))\n",
    "\n",
    "\n",
    "# iterate over tables, lines, and selection marks on each page\n",
    "for page in result.pages:\n",
    "    print(\"\\nLines found on page {}\".format(page.page_number))\n",
    "    for line in page.lines:\n",
    "        print(\"...Line '{}'\".format(line.content.encode('utf-8')))\n",
    "    for word in page.words:\n",
    "        print(\n",
    "            \"...Word '{}' has a confidence of {}\".format(\n",
    "                word.content.encode('utf-8'), word.confidence\n",
    "            )\n",
    "        )\n",
    "    for selection_mark in page.selection_marks:\n",
    "        print(\n",
    "            \"...Selection mark is '{}' and has a confidence of {}\".format(\n",
    "                selection_mark.state, selection_mark.confidence\n",
    "            )\n",
    "        )\n",
    "\n",
    "for i, table in enumerate(result.tables):\n",
    "    print(\"\\nTable {} can be found on page:\".format(i + 1))\n",
    "    for region in table.bounding_regions:\n",
    "        print(\"...{}\".format(i + 1, region.page_number))\n",
    "    for cell in table.cells:\n",
    "        print(\n",
    "            \"...Cell[{}][{}] has content '{}'\".format(\n",
    "                cell.row_index, cell.column_index, cell.content.encode('utf-8')\n",
    "            )\n",
    "        )\n",
    "print(\"-----------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An account SAS is signed with the account access key. The following code example shows \n",
    "# how to call the generate_account_sas method to get the account SAS token string.\n",
    "from datetime import datetime\n",
    "from azure.storage.blob import (\n",
    "    BlobServiceClient,\n",
    "    ContainerClient,\n",
    "    BlobClient,\n",
    "    BlobSasPermissions,\n",
    "    ResourceTypes,\n",
    "    AccountSasPermissions,\n",
    "    UserDelegationKey,\n",
    "    generate_account_sas,\n",
    "    generate_container_sas,\n",
    "    generate_blob_sas\n",
    ")\n",
    "\n",
    "def create_account_sas(self, account_name: str, account_key: str):\n",
    "    # Create an account SAS that's valid for one day\n",
    "    start_time = datetime.datetime.now(datetime.timezone.utc)\n",
    "    expiry_time = start_time + datetime.timedelta(days=1)\n",
    "\n",
    "    # Define the SAS token permissions\n",
    "    sas_permissions=AccountSasPermissions(read=True)\n",
    "\n",
    "    # Define the SAS token resource types\n",
    "    # For this example, we grant access to service-level APIs\n",
    "    sas_resource_types=ResourceTypes(service=True)\n",
    "\n",
    "    sas_token = generate_account_sas(\n",
    "        account_name=account_name,\n",
    "        account_key=account_key,\n",
    "        resource_types=sas_resource_types,\n",
    "        permission=sas_permissions,\n",
    "        expiry=expiry_time,\n",
    "        start=start_time\n",
    "    )\n",
    "\n",
    "    return sas_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract data from forms with Azure Document Intelligence \\\n",
    "https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/concept-custom?view=doc-intel-3.1.0&tabs=fott \\\n",
    "https://github.com/Azure-Samples/cognitive-services-quickstart-code/blob/master/python/FormRecognizer/rest/python-train-extract.md \\\n",
    "Training https://learn.microsoft.com/en-us/training/modules/work-form-recognizer/?source=recommendations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
