{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, OpenAIChatCompletion\n",
    "import os\n",
    "\n",
    "deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "kernel.add_chat_service(                      # We are adding a text service\n",
    "    \"Azure_curie\",                            # The alias we can use in prompt templates' config.json\n",
    "    AzureChatCompletion(\n",
    "        \"my-finetuned-Curie\",                 # Azure OpenAI *Deployment name*\n",
    "        \"https://contoso.openai.azure.com/\",  # Azure OpenAI *Endpoint*\n",
    "        \"...your Azure OpenAI Key...\"         # Azure OpenAI *Key*\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Semantic Functions Inline\n",
    "Try Using ChatCompletion for Semantic Skills which can use Newer models (2023–)\tgpt-4, gpt-4 turbo, gpt-3.5-turbo\\\n",
    "Both AzureTextCompletion and OpenAITextCompletion currently only use Legacy models (2020–2022)\ttext-davinci-003, text-davinci-002, davinci, curie, babbage, ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import semantic_kernel as sk\n",
    "# from semantic_kernel.connectors.ai.open_ai import AzureTextCompletion, OpenAITextCompletion\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, OpenAIChatCompletion\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "# OpenAIChatCompletion is more cheaper than AzureChatCompletion\n",
    "useAzureOpenAI = False\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "org_id = os.getenv(\"OPENAI_ORG_ID\")\n",
    "model = \"text-davinci-003\"\n",
    "\n",
    "# Configure AI service used by the kernel\n",
    "# if useAzureOpenAI:\n",
    "#     # deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "#     kernel.add_text_completion_service(\"dv\", AzureTextCompletion(deployment, endpoint, api_key))\n",
    "# else:\n",
    "#     # api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "#     kernel.add_text_completion_service(\"dv\", OpenAITextCompletion(model, api_key, org_id))\n",
    "\n",
    "# Configure AI service used by the kernel\n",
    "if useAzureOpenAI:\n",
    "    # deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "    kernel.add_chat_service(\n",
    "        \"chat_completion\",\n",
    "        AzureChatCompletion(deployment, endpoint, api_key),\n",
    "    )\n",
    "else:\n",
    "    # api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "    kernel.add_chat_service(\n",
    "        \"chat-gpt\", OpenAIChatCompletion(\"gpt-3.5-turbo\", api_key, org_id)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"{{$input}}\n",
    "Summarize the content above.\n",
    "\"\"\"\n",
    "\n",
    "summarize = kernel.create_semantic_function(prompt, max_tokens=2000, temperature=0.2, top_p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "Demo (ancient Greek poet)\n",
    "From Wikipedia, the free encyclopedia\n",
    "Demo or Damo (Greek: Δεμώ, Δαμώ; fl. c. AD 200) was a Greek woman of the Roman period, known for a single epigram, engraved upon the Colossus of Memnon, which bears her name. She speaks of herself therein as a lyric poetess dedicated to the Muses, but nothing is known of her life.[1]\n",
    "Identity\n",
    "Demo was evidently Greek, as her name, a traditional epithet of Demeter, signifies. The name was relatively common in the Hellenistic world, in Egypt and elsewhere, and she cannot be further identified. The date of her visit to the Colossus of Memnon cannot be established with certainty, but internal evidence on the left leg suggests her poem was inscribed there at some point in or after AD 196.[2]\n",
    "Epigram\n",
    "There are a number of graffiti inscriptions on the Colossus of Memnon. Following three epigrams by Julia Balbilla, a fourth epigram, in elegiac couplets, entitled and presumably authored by \"Demo\" or \"Damo\" (the Greek inscription is difficult to read), is a dedication to the Muses.[2] The poem is traditionally published with the works of Balbilla, though the internal evidence suggests a different author.[1]\n",
    "In the poem, Demo explains that Memnon has shown her special respect. In return, Demo offers the gift for poetry, as a gift to the hero. At the end of this epigram, she addresses Memnon, highlighting his divine status by recalling his strength and holiness.[2]\n",
    "Demo, like Julia Balbilla, writes in the artificial and poetic Aeolic dialect. The language indicates she was knowledgeable in Homeric poetry—'bearing a pleasant gift', for example, alludes to the use of that phrase throughout the Iliad and Odyssey.[a][2] \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, async is available too: summary = await summarize.invoke_async(input_text)\n",
    "summary = summarize(input_text)\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a basic chat experience with context variables\n",
    "environments initialized is on the top paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_prompt = \"\"\"\n",
    "ChatBot can have a conversation with you about any topic.\n",
    "It can give explicit instructions or say 'I don't know' if it does not have an answer.\n",
    "\n",
    "{{$history}}\n",
    "User: {{$user_input}}\n",
    "ChatBot: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register your semantic function\n",
    "chat_function = kernel.create_semantic_function(sk_prompt, \"ChatBot\", max_tokens=2000, temperature=0.7, top_p=0.5)\n",
    "\n",
    "# Initialize your context\n",
    "context = kernel.create_new_context()\n",
    "context[\"history\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat with the Bot\n",
    "context[\"user_input\"] = \"Hi, I'm looking for book suggestions\"\n",
    "bot_answer = None\n",
    "bot_answer = await chat_function.invoke_async(context=context)\n",
    "print(bot_answer)\n",
    "\n",
    "continue_flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "if continue_flag:\n",
    "    def show_json(obj):\n",
    "        display(json.loads(obj.json())) \n",
    "\n",
    "    # Pretty printing helper\n",
    "    def pretty_print(messages):\n",
    "        print(\"# Messages\")\n",
    "        for m in messages:\n",
    "            print(f\"{m.role}: {m.content[0].text.value}\")\n",
    "        print()\n",
    "\n",
    "    show_json(bot_answer)\n",
    "    # print(bot_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the history with the output\n",
    "context[\"history\"] += f\"\\nUser: {context['user_input']}\\nChatBot: {bot_answer}\\n\"\n",
    "print(context[\"history\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep Chatting!\n",
    "async def chat(input_text: str) -> None:\n",
    "    # Save new message in the context variables\n",
    "    print(f\"User: {input_text}\")\n",
    "    context[\"user_input\"] = input_text\n",
    "\n",
    "    # Process the user message and get an answer\n",
    "    answer = await chat_function.invoke_async(context=context)\n",
    "\n",
    "    # Show the response\n",
    "    print(f\"ChatBot: {answer}\")\n",
    "\n",
    "    # Append the new interaction to the chat history\n",
    "    context[\"history\"] += f\"\\nUser: {input_text}\\nChatBot: {answer}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await chat(\"I love history and philosophy, I'd like to learn something new about Greece, any suggestion?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await chat(\"that sounds interesting, what is it about?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await chat(\"if I read that book, what exactly will I learn about Greek history?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await chat(\"could you list some more books I could read about this topic?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(context[\"history\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to the Planner\n",
    "planner works best with more powerful models like gpt4 but sometimes you might get working plans with cheaper models like gpt-35-turbo, since planner will output json or xml format and gpt4 can handle that perfectly\\\n",
    "examples are available in 08-using-the-planner.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Semantic Memory with Embeddings\n",
    "If prompt would grow so large that you would run into a the model's token limit. What we need is a way to persist state and build both short-term and long-term memory to empower even more intelligent applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import semantic_kernel as sk\n",
    "from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion, OpenAITextEmbedding, AzureChatCompletion, AzureTextEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use memory, we need to instantiate the Kernel with a Memory Storage and an Embedding service. In this example, we make use of the VolatileMemoryStore \"which can be thought of as a temporary in-memory storage (not to be confused with Semantic Memory). This memory is not written to disk and is only available during the app session.\n",
    "\n",
    "When developing your app you will have the option to plug in persistent storage like Azure Cosmos Db, PostgreSQL, SQLite, etc. Semantic Memory allows also to index external data sources, without duplicating all the information, more on that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recall': <semantic_kernel.orchestration.sk_function.SKFunction at 0x190d18330d0>,\n",
       " 'save': <semantic_kernel.orchestration.sk_function.SKFunction at 0x190d1833550>}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel = sk.Kernel()\n",
    "\n",
    "useAzureOpenAI = False\n",
    "\n",
    "# Configure AI service used by the kernel\n",
    "if useAzureOpenAI:\n",
    "    # deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "    kernel.add_chat_service(\"chat_completion\", AzureChatCompletion(deployment, endpoint, api_key))\n",
    "    # next line assumes embeddings deployment name is \"text-embedding-ada-002\", adjust this if  appropriate \n",
    "    kernel.add_text_embedding_generation_service(\"ada\", AzureTextEmbedding(\"text-embedding-ada-002\", endpoint, api_key))\n",
    "else:\n",
    "    # api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "    kernel.add_chat_service(\"chat-gpt\", OpenAIChatCompletion(\"gpt-3.5-turbo\", api_key, org_id))\n",
    "    kernel.add_text_embedding_generation_service(\"ada\", OpenAITextEmbedding(\"text-embedding-ada-002\", api_key, org_id))\n",
    "\n",
    "kernel.register_memory_store(memory_store=sk.memory.VolatileMemoryStore())\n",
    "kernel.import_skill(sk.core_skills.TextMemorySkill())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At its core, Semantic Memory is a set of data structures that allow you to store the meaning of text that come from different data sources, and optionally to store the source text too. These texts can be from the web, e-mail providers, chats, a database, or from your local directory, and are hooked up to the Semantic Kernel through data source connectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create some initial memories \"About Me\". \n",
    "# We can add memories to our VolatileMemoryStore by using SaveInformationAsync\n",
    "async def populate_memory(kernel: sk.Kernel) -> None:\n",
    "    # Add some documents to the semantic memory\n",
    "    await kernel.memory.save_information_async(\n",
    "        \"aboutMe\", id=\"info1\", text=\"My name is Andrea\"\n",
    "    )\n",
    "    # await kernel.memory.save_information_async(\n",
    "    #     \"aboutMe\", id=\"info2\", text=\"I currently work as a tour guide\"\n",
    "    # )\n",
    "    # await kernel.memory.save_information_async(\n",
    "    #     \"aboutMe\", id=\"info3\", text=\"I've been living in Seattle since 2005\"\n",
    "    # )\n",
    "    # await kernel.memory.save_information_async(\n",
    "    #     \"aboutMe\", id=\"info4\", text=\"I visited France and Italy five times since 2015\"\n",
    "    # )\n",
    "    # await kernel.memory.save_information_async(\n",
    "    #     \"aboutMe\", id=\"info5\", text=\"My family is from New York\"\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try searching the memory:\n",
    "async def search_memory_examples(kernel: sk.Kernel) -> None:\n",
    "    questions = [\n",
    "        \"what's my name\",\n",
    "        \"where do I live?\",\n",
    "        \"where's my family from?\",\n",
    "        \"where have I traveled?\",\n",
    "        \"what do I do for work\",\n",
    "    ]\n",
    "\n",
    "    for question in questions:\n",
    "        print(f\"Question: {question}\")\n",
    "        result = await kernel.memory.search_async(\"aboutMe\", question)\n",
    "        print(f\"Answer: {result[0].text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More steps are found in https://github.com/microsoft/semantic-kernel/blob/main/python/notebooks/06-memory-and-embeddings.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Native Functions\n",
    "This can be useful in a few scenarios:\n",
    "\n",
    "* Writing logic around how to run a prompt that changes the prompt's outcome.\n",
    "* Using external data sources to gather data to concatenate into your prompt.\n",
    "* Validating user input data prior to sending it to the LLM prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from semantic_kernel.skill_definition import sk_function\n",
    "\n",
    "class GenerateNumberSkill:\n",
    "    \"\"\"\n",
    "    Description: Generate a number between 3-x.\n",
    "    \"\"\"\n",
    "\n",
    "    @sk_function(\n",
    "        description=\"Generate a random number between 3-x\",\n",
    "        name=\"GenerateNumberThreeOrHigher\"\n",
    "    )\n",
    "    def generate_number_three_or_higher(self, input: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a number between 3-<input>\n",
    "        Example:\n",
    "            \"8\" => rand(3,8)\n",
    "        Args:\n",
    "            input -- The upper limit for the random number generation\n",
    "        Returns:\n",
    "            int value\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"inside function\") # debug print\n",
    "            return str(random.randint(3, int(input))) \n",
    "        except ValueError as e:\n",
    "            print(f\"Invalid input {input}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, let's create a semantic function that accepts a number as {{$input}} \n",
    "# and generates that number of paragraphs about two Corgis on an adventure. \n",
    "# $input is a default variable semantic functions can use.\n",
    "sk_prompt = \"\"\"\n",
    "Write a short story about two Corgis on an adventure.\n",
    "The story must be:\n",
    "- G rated\n",
    "- Have a positive message\n",
    "- No sexism, racism or other bias/bigotry\n",
    "- Be exactly {{$input}} paragraphs long\n",
    "\"\"\"\n",
    "\n",
    "corgi_story = kernel.create_semantic_function(prompt_template=sk_prompt,\n",
    "                                              function_name=\"CorgiStory\",\n",
    "                                              skill_name=\"CorgiSkill\",\n",
    "                                              description=\"Write a short story about two Corgis on an adventure\",\n",
    "                                              max_tokens=500,\n",
    "                                              temperature=0.5,\n",
    "                                              top_p=0.5)\n",
    "\n",
    "generate_number_skill = kernel.import_skill(GenerateNumberSkill())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside function\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Run the number generator\n",
    "generate_number_three_or_higher = generate_number_skill[\"GenerateNumberThreeOrHigher\"]\n",
    "number_result = generate_number_three_or_higher(6)\n",
    "print(number_result)\n",
    "# generate_number_three_or_higher.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a corgi story exactly 5 paragraphs long: \n",
      "=====================================================\n",
      "Once upon a time in a cozy little town, there lived two adorable Corgis named Charlie and Daisy. They were the best of friends and loved going on adventures together. One sunny morning, they decided to explore the nearby forest, filled with tall trees and colorful flowers.\n",
      "\n",
      "As they trotted along the winding path, Charlie and Daisy noticed a lost butterfly fluttering around in confusion. Worried, they approached the butterfly and asked if it needed help. The butterfly explained that it had lost its way home and was feeling scared.\n",
      "\n",
      "Without hesitation, Charlie and Daisy offered their assistance. They led the butterfly through the forest, using their keen sense of smell and sharp eyes to find familiar landmarks. Along the way, they encountered other animals who joined their mission, creating a little parade of kindness.\n",
      "\n",
      "Finally, they reached a beautiful meadow where the butterfly's family eagerly awaited its return. The butterfly thanked Charlie and Daisy, expressing its gratitude for their selflessness and friendship. The two Corgis wagged their tails happily, knowing they had made a difference in someone's life.\n",
      "\n",
      "As they made their way back home, Charlie and Daisy reflected on their adventure. They realized that even the smallest acts of kindness could have a big impact on others. They vowed to always be there for those in need, just like they had been for the lost butterfly.\n",
      "\n",
      "From that day forward, Charlie and Daisy became known as the town's heroes, always ready to lend a helping paw. Their story spread far and wide, inspiring others to follow their example. And so, the little town became a place filled with compassion, where everyone lived happily ever after.\n"
     ]
    }
   ],
   "source": [
    "story = await corgi_story.invoke_async(input=number_result.result)\n",
    "print(\"Generating a corgi story exactly {} paragraphs long: \".format(number_result.result))\n",
    "print(\"=====================================================\")\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context Variables\n",
    "That works! But let's expand on our example to make it more generic.\n",
    "\n",
    "For the native function, we'll introduce the lower limit variable. This means that a user will input two numbers and the number generator function will pick a number between the first and second input.\n",
    "\n",
    "We'll make use of <strong>the semantic_kernel.ContextVariables class</strong> to do hold these variables.\n",
    "\n",
    "Let's start with the native function. Notice that we're also adding @sk_function_context_parameter decorators to the function here to provide context about what variables need to be provided to the function, and any defaults for those inputs. Using the @sk_function_context_parameter decorator provides the name, description and default values for a function's inputs to the planner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from semantic_kernel.skill_definition import sk_function, sk_function_context_parameter\n",
    "from semantic_kernel import SKContext\n",
    "\n",
    "class GenerateNumberSkill:\n",
    "    \"\"\"\n",
    "    Description: Generate a number between a min and a max.\n",
    "    \"\"\"\n",
    "\n",
    "    @sk_function(\n",
    "        description=\"Generate a random number between min and max\",\n",
    "        name=\"GenerateNumber\"\n",
    "    )\n",
    "    @sk_function_context_parameter(name=\"min\", description=\"Minimum number of paragraphs.\")\n",
    "    @sk_function_context_parameter(name=\"max\", description=\"Maximum number of paragraphs.\", default_value=10)\n",
    "    def generate_number(self, context: SKContext) -> str:\n",
    "        \"\"\"\n",
    "        Generate a number between min-max\n",
    "        Example:\n",
    "            min=\"4\" max=\"10\" => rand(4,8)\n",
    "        Args:\n",
    "            min -- The lower limit for the random number generation\n",
    "            max -- The upper limit for the random number generation\n",
    "        Returns:\n",
    "            int value\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return str(random.randint(int(context[\"min\"]), int(context[\"max\"]))) \n",
    "        except ValueError as e:\n",
    "            print(f\"Invalid input {context['min']} {context['max']}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_number_skill = kernel.import_skill(GenerateNumberSkill())\n",
    "generate_number = generate_number_skill[\"GenerateNumber\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_prompt = \"\"\"\n",
    "Write a short story about two Corgis on an adventure.\n",
    "The story must be:\n",
    "- G rated\n",
    "- Have a positive message\n",
    "- No sexism, racism or other bias/bigotry\n",
    "- Be exactly {{$paragraph_count}} paragraphs long\n",
    "- Be written in this language: {{$language}}\n",
    "\"\"\"\n",
    "\n",
    "corgi_story = kernel.create_semantic_function(prompt_template=sk_prompt,\n",
    "                                              function_name=\"CorgiStory\",\n",
    "                                              skill_name=\"CorgiSkill\",\n",
    "                                              description=\"Write a short story about two Corgis on an adventure\",\n",
    "                                              max_tokens=500,\n",
    "                                              temperature=0.5,\n",
    "                                              top_p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a corgi story exactly 4 paragraphs long in Chinese language: \n",
      "=====================================================\n",
      "从小到大，小短腿和小长腿一直是最好的朋友。他们都是可爱的柯基犬，每天都在花园里一起玩耍。然而，他们总是梦想着有一天能够去探险，看看世界的其他地方。\n",
      "\n",
      "有一天，他们决定实现自己的梦想。小短腿和小长腿穿过了大草原，跳过了小溪，来到了一个陌生的森林。他们充满好奇地探索着，发现了一只迷路的小松鼠。小短腿和小长腿决定帮助它找到回家的路。\n",
      "\n",
      "他们穿过了茂密的树林，跳过了高高的石头，终于找到了小松鼠的家。小松鼠非常感激，它邀请小短腿和小长腿在森林里玩耍。他们一起玩耍，跳跃，享受着快乐的时光。\n",
      "\n",
      "当天晚上，小短腿和小长腿回到了自己的家。他们感到非常满足和幸福，因为他们不仅实现了自己的梦想，还帮助了别人。他们明白到，友谊和善良是最重要的事情，无论身处何地。\n",
      "\n",
      "从那以后，小短腿和小长腿成为了森林和花园之间的桥梁。他们带着快乐和友谊的故事，与其他动物分享。他\n"
     ]
    }
   ],
   "source": [
    "context_variables = sk.ContextVariables(variables={\n",
    "    \"min\": 1,\n",
    "    \"max\": 5,\n",
    "    \"language\": \"Chinese\",\n",
    "})\n",
    "\n",
    "context_variables['paragraph_count'] = generate_number.invoke(variables=context_variables).result\n",
    "\n",
    "# Pass the output to the semantic story function\n",
    "story = await corgi_story.invoke_async(variables=context_variables)\n",
    "\n",
    "print(\"Generating a corgi story exactly {} paragraphs long in {} language: \".format(context_variables[\"paragraph_count\"],\n",
    "                                                                                    context_variables[\"language\"]))\n",
    "print(\"=====================================================\")\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calling Native Functions within a Semantic Function\n",
    "One neat thing about the Semantic Kernel is that you can also call native functions from within Semantic Functions!\n",
    "\n",
    "We will make our CorgiStory semantic function call a native function GenerateNames which will return names for our Corgi characters.\n",
    "\n",
    "We do this using the syntax {{skill_name.function_name}}. You can read more about our prompte templating syntax here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import semantic_kernel as sk\n",
    "# from semantic_kernel.connectors.ai.open_ai import AzureTextCompletion, OpenAITextCompletion\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, OpenAIChatCompletion\n",
    "\n",
    "kernel = sk.Kernel()\n",
    "\n",
    "# OpenAIChatCompletion is more cheaper than AzureChatCompletion\n",
    "useAzureOpenAI = False\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "org_id = os.getenv(\"OPENAI_ORG_ID\")\n",
    "model = \"text-davinci-003\"\n",
    "\n",
    "# Configure AI service used by the kernel\n",
    "# if useAzureOpenAI:\n",
    "#     # deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "#     kernel.add_text_completion_service(\"dv\", AzureTextCompletion(deployment, endpoint, api_key))\n",
    "# else:\n",
    "#     # api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "#     kernel.add_text_completion_service(\"dv\", OpenAITextCompletion(model, api_key, org_id))\n",
    "\n",
    "# Configure AI service used by the kernel\n",
    "if useAzureOpenAI:\n",
    "    # deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()\n",
    "    kernel.add_chat_service(\n",
    "        \"chat_completion\",\n",
    "        AzureChatCompletion(deployment, endpoint, api_key),\n",
    "    )\n",
    "else:\n",
    "    # api_key, org_id = sk.openai_settings_from_dot_env()\n",
    "    kernel.add_chat_service(\n",
    "        \"chat-gpt\", OpenAIChatCompletion(\"gpt-3.5-turbo\", api_key, org_id)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from semantic_kernel.skill_definition import sk_function\n",
    "\n",
    "\n",
    "class GenerateNamesSkill:\n",
    "    \"\"\"\n",
    "    Description: Generate character names.\n",
    "    \"\"\"\n",
    "\n",
    "    # The default function name will be the name of the function itself, however you can override this\n",
    "    # by setting the name=<name override> in the @sk_function decorator. In this case, we're using\n",
    "    # the same name as the function name for simplicity.\n",
    "    @sk_function(\n",
    "        description=\"Generate character names\",\n",
    "        name=\"generate_names\"\n",
    "    )\n",
    "    def generate_names(self) -> str:\n",
    "        \"\"\"\n",
    "        Generate two names.\n",
    "        Returns:\n",
    "            str\n",
    "        \"\"\"\n",
    "        names = {\n",
    "            \"Hoagie\",\n",
    "            \"Hamilton\",\n",
    "            \"Bacon\",\n",
    "            \"Pizza\",\n",
    "            \"Boots\",\n",
    "            \"Shorts\",\n",
    "            \"Tuna\"\n",
    "        }\n",
    "        first_name = random.choice(list(names))\n",
    "        names.remove(first_name)\n",
    "        second_name = random.choice(list(names))\n",
    "        return f\"{first_name}, {second_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_names_skill = kernel.import_skill(GenerateNamesSkill(), skill_name=\"GenerateNames\")\n",
    "generate_names = generate_names_skill[\"generate_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_prompt = \"\"\"\n",
    "Write a short story about two Corgis on an adventure.\n",
    "The story must be:\n",
    "- G rated\n",
    "- Have a positive message\n",
    "- No sexism, racism or other bias/bigotry\n",
    "- Be exactly {{$paragraph_count}} paragraphs long\n",
    "- Be written in this language: {{$language}}\n",
    "- The two names of the corgis are {{GenerateNames.generate_names}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "corgi_story = kernel.create_semantic_function(prompt_template=sk_prompt,\n",
    "                                              function_name=\"CorgiStory\",\n",
    "                                              skill_name=\"CorgiSkill\",\n",
    "                                              description=\"Write a short story about two Corgis on an adventure\",\n",
    "                                              max_tokens=500,\n",
    "                                              temperature=0.5,\n",
    "                                              top_p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a corgi story exactly 2 paragraphs long in Chinese language: \n",
      "=====================================================\n",
      "Bacon和Tuna是两只可爱的柯基犬，它们是最好的朋友。一天，它们决定展开一次冒险之旅。它们在阳光明媚的早晨出发了。\n",
      "\n",
      "它们穿过了郁郁葱葱的森林，跳过了清澈的小溪，一直走到了一个美丽的花园。在那里，它们看到了一只小鸟被困在了网中。Bacon和Tuna立刻决定帮助它。它们小心翼翼地咬断了网子，小鸟终于自由了。小鸟欢快地唱起歌来，感激地飞走了。\n",
      "\n",
      "Bacon和Tuna感到非常快乐和满足。它们明白了一个重要的道理：帮助别人会带来快乐。它们继续它们的冒险之旅，希望能够帮助更多的朋友。从那天起，Bacon和Tuna成为了这个小镇上最受欢迎的狗狗，它们的友谊和善良的行为一直鼓舞着大家。\n"
     ]
    }
   ],
   "source": [
    "context_variables = sk.ContextVariables(variables={\n",
    "    \"min\": 1,\n",
    "    \"max\": 5,\n",
    "    \"language\": \"Chinese\",\n",
    "})\n",
    "context_variables['paragraph_count'] = generate_number.invoke(variables=context_variables).result\n",
    "\n",
    "# Pass the output to the semantic story function\n",
    "story = await corgi_story.invoke_async(variables=context_variables)\n",
    "\n",
    "print(\"Generating a corgi story exactly {} paragraphs long in {} language: \".format(context_variables[\"paragraph_count\"],\n",
    "                                                                                    context_variables[\"language\"]))\n",
    "print(\"=====================================================\")\n",
    "print(story)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
